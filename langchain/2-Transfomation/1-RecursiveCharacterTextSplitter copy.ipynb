{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. How to recursively split text by characters\n",
    "\n",
    "> `Reference` - https://python.langchain.com/docs/how_to/recursive_text_splitter/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'docs/attention.pdf', 'page': 0}, page_content='Provided proper attribution is provided, Google hereby grants permission to'), Document(metadata={'source': 'docs/attention.pdf', 'page': 0}, page_content='reproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.'), Document(metadata={'source': 'docs/attention.pdf', 'page': 0}, page_content='scholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain'), Document(metadata={'source': 'docs/attention.pdf', 'page': 0}, page_content='Google Brain\\navaswani@google.comNoam Shazeer∗\\nGoogle Brain\\nnoam@google.comNiki Parmar∗'), Document(metadata={'source': 'docs/attention.pdf', 'page': 0}, page_content='Google Research\\nnikip@google.comJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗'), Document(metadata={'source': 'docs/attention.pdf', 'page': 0}, page_content='Llion Jones∗\\nGoogle Research\\nllion@google.comAidan N. Gomez∗ †\\nUniversity of Toronto'), Document(metadata={'source': 'docs/attention.pdf', 'page': 0}, page_content='aidan@cs.toronto.eduŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡'), Document(metadata={'source': 'docs/attention.pdf', 'page': 0}, page_content='Illia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract'), Document(metadata={'source': 'docs/attention.pdf', 'page': 0}, page_content='Abstract\\nThe dominant sequence transduction models are based on complex recurrent or'), Document(metadata={'source': 'docs/attention.pdf', 'page': 0}, page_content='convolutional neural networks that include an encoder and a decoder. The best'), Document(metadata={'source': 'docs/attention.pdf', 'page': 0}, page_content='performing models also connect the encoder and decoder through an attention'), Document(metadata={'source': 'docs/attention.pdf', 'page': 0}, page_content='mechanism. We propose a new simple network architecture, the Transformer,'), Document(metadata={'source': 'docs/attention.pdf', 'page': 0}, page_content='based solely on attention mechanisms, dispensing with recurrence and convolutions'), Document(metadata={'source': 'docs/attention.pdf', 'page': 0}, page_content='entirely. Experiments on two machine translation tasks show these models to'), Document(metadata={'source': 'docs/attention.pdf', 'page': 0}, page_content='be superior in quality while being more parallelizable and requiring significantly'), Document(metadata={'source': 'docs/attention.pdf', 'page': 0}, page_content='less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-'), Document(metadata={'source': 'docs/attention.pdf', 'page': 0}, page_content='to-German translation task, improving over the existing best results, including'), Document(metadata={'source': 'docs/attention.pdf', 'page': 0}, page_content='ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,'), Document(metadata={'source': 'docs/attention.pdf', 'page': 0}, page_content='our model establishes a new single-model state-of-the-art BLEU score of 41.8 after'), Document(metadata={'source': 'docs/attention.pdf', 'page': 0}, page_content='training for 3.5 days on eight GPUs, a small fraction of the training costs of the'), Document(metadata={'source': 'docs/attention.pdf', 'page': 0}, page_content='best models from the literature. We show that the Transformer generalizes well to'), Document(metadata={'source': 'docs/attention.pdf', 'page': 0}, page_content='other tasks by applying it successfully to English constituency parsing both with'), Document(metadata={'source': 'docs/attention.pdf', 'page': 0}, page_content='large and limited training data.'), Document(metadata={'source': 'docs/attention.pdf', 'page': 0}, page_content='∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and'), Document(metadata={'source': 'docs/attention.pdf', 'page': 0}, page_content='self-attention and started'), Document(metadata={'source': 'docs/attention.pdf', 'page': 0}, page_content='the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first'), Document(metadata={'source': 'docs/attention.pdf', 'page': 0}, page_content='the first Transformer models and'), Document(metadata={'source': 'docs/attention.pdf', 'page': 0}, page_content='has been crucially involved in every aspect of this work. Noam proposed scaled dot-product'), Document(metadata={'source': 'docs/attention.pdf', 'page': 0}, page_content='scaled dot-product attention, multi-head'), Document(metadata={'source': 'docs/attention.pdf', 'page': 0}, page_content='attention and the parameter-free position representation and became the other person involved in'), Document(metadata={'source': 'docs/attention.pdf', 'page': 0}, page_content='person involved in nearly every'), Document(metadata={'source': 'docs/attention.pdf', 'page': 0}, page_content='detail. Niki designed, implemented, tuned and evaluated countless model variants in our original'), Document(metadata={'source': 'docs/attention.pdf', 'page': 0}, page_content='in our original codebase and'), Document(metadata={'source': 'docs/attention.pdf', 'page': 0}, page_content='tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial'), Document(metadata={'source': 'docs/attention.pdf', 'page': 0}, page_content='for our initial codebase, and'), Document(metadata={'source': 'docs/attention.pdf', 'page': 0}, page_content='efficient inference and visualizations. Lukasz and Aidan spent countless long days designing'), Document(metadata={'source': 'docs/attention.pdf', 'page': 0}, page_content='long days designing various parts of and'), Document(metadata={'source': 'docs/attention.pdf', 'page': 0}, page_content='implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively'), Document(metadata={'source': 'docs/attention.pdf', 'page': 0}, page_content='and massively accelerating'), Document(metadata={'source': 'docs/attention.pdf', 'page': 0}, page_content='our research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.'), Document(metadata={'source': 'docs/attention.pdf', 'page': 0}, page_content='31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA,'), Document(metadata={'source': 'docs/attention.pdf', 'page': 0}, page_content='Long Beach, CA, USA.arXiv:1706.03762v7  [cs.CL]  2 Aug 2023'), Document(metadata={'source': 'docs/attention.pdf', 'page': 1}, page_content='1 Introduction'), Document(metadata={'source': 'docs/attention.pdf', 'page': 1}, page_content='Recurrent neural networks, long short-term memory [ 13] and gated recurrent [ 7] neural networks'), Document(metadata={'source': 'docs/attention.pdf', 'page': 1}, page_content='in particular, have been firmly established as state of the art approaches in sequence modeling and'), Document(metadata={'source': 'docs/attention.pdf', 'page': 1}, page_content='transduction problems such as language modeling and machine translation [ 35,2,5]. Numerous'), Document(metadata={'source': 'docs/attention.pdf', 'page': 1}, page_content='efforts have since continued to push the boundaries of recurrent language models and'), Document(metadata={'source': 'docs/attention.pdf', 'page': 1}, page_content='language models and encoder-decoder'), Document(metadata={'source': 'docs/attention.pdf', 'page': 1}, page_content='architectures [38, 24, 15].'), Document(metadata={'source': 'docs/attention.pdf', 'page': 1}, page_content='Recurrent models typically factor computation along the symbol positions of the input and output'), Document(metadata={'source': 'docs/attention.pdf', 'page': 1}, page_content='sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden'), Document(metadata={'source': 'docs/attention.pdf', 'page': 1}, page_content='states ht, as a function of the previous hidden state ht−1and the input for position t. This'), Document(metadata={'source': 'docs/attention.pdf', 'page': 1}, page_content='position t. This inherently'), Document(metadata={'source': 'docs/attention.pdf', 'page': 1}, page_content='sequential nature precludes parallelization within training examples, which becomes critical at'), Document(metadata={'source': 'docs/attention.pdf', 'page': 1}, page_content='becomes critical at longer'), Document(metadata={'source': 'docs/attention.pdf', 'page': 1}, page_content='sequence lengths, as memory constraints limit batching across examples. Recent work has achieved'), Document(metadata={'source': 'docs/attention.pdf', 'page': 1}, page_content='significant improvements in computational efficiency through factorization tricks [ 21] and'), Document(metadata={'source': 'docs/attention.pdf', 'page': 1}, page_content='tricks [ 21] and conditional'), Document(metadata={'source': 'docs/attention.pdf', 'page': 1}, page_content='computation [ 32], while also improving model performance in case of the latter. The fundamental'), Document(metadata={'source': 'docs/attention.pdf', 'page': 1}, page_content='constraint of sequential computation, however, remains.'), Document(metadata={'source': 'docs/attention.pdf', 'page': 1}, page_content='Attention mechanisms have become an integral part of compelling sequence modeling and transduc-'), Document(metadata={'source': 'docs/attention.pdf', 'page': 1}, page_content='tion models in various tasks, allowing modeling of dependencies without regard to their distance in'), Document(metadata={'source': 'docs/attention.pdf', 'page': 1}, page_content='the input or output sequences [ 2,19]. In all but a few cases [ 27], however, such attention'), Document(metadata={'source': 'docs/attention.pdf', 'page': 1}, page_content='such attention mechanisms'), Document(metadata={'source': 'docs/attention.pdf', 'page': 1}, page_content='are used in conjunction with a recurrent network.'), Document(metadata={'source': 'docs/attention.pdf', 'page': 1}, page_content='In this work we propose the Transformer, a model architecture eschewing recurrence and instead'), Document(metadata={'source': 'docs/attention.pdf', 'page': 1}, page_content='relying entirely on an attention mechanism to draw global dependencies between input and output.'), Document(metadata={'source': 'docs/attention.pdf', 'page': 1}, page_content='The Transformer allows for significantly more parallelization and can reach a new state of the art'), Document(metadata={'source': 'docs/attention.pdf', 'page': 1}, page_content='state of the art in'), Document(metadata={'source': 'docs/attention.pdf', 'page': 1}, page_content='translation quality after being trained for as little as twelve hours on eight P100 GPUs.'), Document(metadata={'source': 'docs/attention.pdf', 'page': 1}, page_content='2 Background'), Document(metadata={'source': 'docs/attention.pdf', 'page': 1}, page_content='The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU'), Document(metadata={'source': 'docs/attention.pdf', 'page': 1}, page_content='[16], ByteNet [ 18] and ConvS2S [ 9], all of which use convolutional neural networks as basic'), Document(metadata={'source': 'docs/attention.pdf', 'page': 1}, page_content='networks as basic building'), Document(metadata={'source': 'docs/attention.pdf', 'page': 1}, page_content='block, computing hidden representations in parallel for all input and output positions. In these'), Document(metadata={'source': 'docs/attention.pdf', 'page': 1}, page_content='positions. In these models,'), Document(metadata={'source': 'docs/attention.pdf', 'page': 1}, page_content='the number of operations required to relate signals from two arbitrary input or output positions'), Document(metadata={'source': 'docs/attention.pdf', 'page': 1}, page_content='or output positions grows'), Document(metadata={'source': 'docs/attention.pdf', 'page': 1}, page_content='in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes'), Document(metadata={'source': 'docs/attention.pdf', 'page': 1}, page_content='it more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is'), Document(metadata={'source': 'docs/attention.pdf', 'page': 1}, page_content='reduced to a constant number of operations, albeit at the cost of reduced effective resolution due'), Document(metadata={'source': 'docs/attention.pdf', 'page': 1}, page_content='to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as'), Document(metadata={'source': 'docs/attention.pdf', 'page': 1}, page_content='described in section 3.2.'), Document(metadata={'source': 'docs/attention.pdf', 'page': 1}, page_content='Self-attention, sometimes called intra-attention is an attention mechanism relating different'), Document(metadata={'source': 'docs/attention.pdf', 'page': 1}, page_content='relating different positions'), Document(metadata={'source': 'docs/attention.pdf', 'page': 1}, page_content='of a single sequence in order to compute a representation of the sequence. Self-attention has been'), Document(metadata={'source': 'docs/attention.pdf', 'page': 1}, page_content='used successfully in a variety of tasks including reading comprehension, abstractive summarization,'), Document(metadata={'source': 'docs/attention.pdf', 'page': 1}, page_content='textual entailment and learning task-independent sentence representations [4, 27, 28, 22].'), Document(metadata={'source': 'docs/attention.pdf', 'page': 1}, page_content='End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-'), Document(metadata={'source': 'docs/attention.pdf', 'page': 1}, page_content='aligned recurrence and have been shown to perform well on simple-language question answering and'), Document(metadata={'source': 'docs/attention.pdf', 'page': 1}, page_content='language modeling tasks [34].'), Document(metadata={'source': 'docs/attention.pdf', 'page': 1}, page_content='To the best of our knowledge, however, the Transformer is the first transduction model relying'), Document(metadata={'source': 'docs/attention.pdf', 'page': 1}, page_content='entirely on self-attention to compute representations of its input and output without using'), Document(metadata={'source': 'docs/attention.pdf', 'page': 1}, page_content='without using sequence-'), Document(metadata={'source': 'docs/attention.pdf', 'page': 1}, page_content='aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate'), Document(metadata={'source': 'docs/attention.pdf', 'page': 1}, page_content='self-attention and discuss its advantages over models such as [17, 18] and [9].'), Document(metadata={'source': 'docs/attention.pdf', 'page': 1}, page_content='3 Model Architecture'), Document(metadata={'source': 'docs/attention.pdf', 'page': 1}, page_content='Most competitive neural sequence transduction models have an encoder-decoder structure [ 5,2,35].'), Document(metadata={'source': 'docs/attention.pdf', 'page': 1}, page_content='Here, the encoder maps an input sequence of symbol representations (x1, ..., x n)to a sequence'), Document(metadata={'source': 'docs/attention.pdf', 'page': 1}, page_content='of continuous representations z= (z1, ..., z n). Given z, the decoder then generates an output'), Document(metadata={'source': 'docs/attention.pdf', 'page': 1}, page_content='sequence (y1, ..., y m)of symbols one element at a time. At each step the model is auto-regressive'), Document(metadata={'source': 'docs/attention.pdf', 'page': 1}, page_content='[10], consuming the previously generated symbols as additional input when generating the next.\\n2'), Document(metadata={'source': 'docs/attention.pdf', 'page': 2}, page_content='Figure 1: The Transformer - model architecture.'), Document(metadata={'source': 'docs/attention.pdf', 'page': 2}, page_content='The Transformer follows this overall architecture using stacked self-attention and point-wise,'), Document(metadata={'source': 'docs/attention.pdf', 'page': 2}, page_content='and point-wise, fully'), Document(metadata={'source': 'docs/attention.pdf', 'page': 2}, page_content='connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,'), Document(metadata={'source': 'docs/attention.pdf', 'page': 2}, page_content='respectively.\\n3.1 Encoder and Decoder Stacks'), Document(metadata={'source': 'docs/attention.pdf', 'page': 2}, page_content='Encoder: The encoder is composed of a stack of N= 6 identical layers. Each layer has two'), Document(metadata={'source': 'docs/attention.pdf', 'page': 2}, page_content='sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple,'), Document(metadata={'source': 'docs/attention.pdf', 'page': 2}, page_content='second is a simple, position-'), Document(metadata={'source': 'docs/attention.pdf', 'page': 2}, page_content='wise fully connected feed-forward network. We employ a residual connection [ 11] around each of'), Document(metadata={'source': 'docs/attention.pdf', 'page': 2}, page_content='the two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is'), Document(metadata={'source': 'docs/attention.pdf', 'page': 2}, page_content='LayerNorm( x+ Sublayer( x)), where Sublayer( x)is the function implemented by the sub-layer'), Document(metadata={'source': 'docs/attention.pdf', 'page': 2}, page_content='itself. To facilitate these residual connections, all sub-layers in the model, as well as the'), Document(metadata={'source': 'docs/attention.pdf', 'page': 2}, page_content='as well as the embedding'), Document(metadata={'source': 'docs/attention.pdf', 'page': 2}, page_content='layers, produce outputs of dimension dmodel = 512 .'), Document(metadata={'source': 'docs/attention.pdf', 'page': 2}, page_content='Decoder: The decoder is also composed of a stack of N= 6identical layers. In addition to the two'), Document(metadata={'source': 'docs/attention.pdf', 'page': 2}, page_content='sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head'), Document(metadata={'source': 'docs/attention.pdf', 'page': 2}, page_content='attention over the output of the encoder stack. Similar to the encoder, we employ residual'), Document(metadata={'source': 'docs/attention.pdf', 'page': 2}, page_content='we employ residual connections'), Document(metadata={'source': 'docs/attention.pdf', 'page': 2}, page_content='around each of the sub-layers, followed by layer normalization. We also modify the self-attention'), Document(metadata={'source': 'docs/attention.pdf', 'page': 2}, page_content='sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This'), Document(metadata={'source': 'docs/attention.pdf', 'page': 2}, page_content='masking, combined with fact that the output embeddings are offset by one position, ensures that the'), Document(metadata={'source': 'docs/attention.pdf', 'page': 2}, page_content='predictions for position ican depend only on the known outputs at positions less than i.'), Document(metadata={'source': 'docs/attention.pdf', 'page': 2}, page_content='3.2 Attention'), Document(metadata={'source': 'docs/attention.pdf', 'page': 2}, page_content='An attention function can be described as mapping a query and a set of key-value pairs to an'), Document(metadata={'source': 'docs/attention.pdf', 'page': 2}, page_content='pairs to an output,'), Document(metadata={'source': 'docs/attention.pdf', 'page': 2}, page_content='where the query, keys, values, and output are all vectors. The output is computed as a weighted sum'), Document(metadata={'source': 'docs/attention.pdf', 'page': 2}, page_content='3'), Document(metadata={'source': 'docs/attention.pdf', 'page': 3}, page_content='Scaled Dot-Product Attention\\n Multi-Head Attention'), Document(metadata={'source': 'docs/attention.pdf', 'page': 3}, page_content='Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several'), Document(metadata={'source': 'docs/attention.pdf', 'page': 3}, page_content='attention layers running in parallel.'), Document(metadata={'source': 'docs/attention.pdf', 'page': 3}, page_content='of the values, where the weight assigned to each value is computed by a compatibility function of'), Document(metadata={'source': 'docs/attention.pdf', 'page': 3}, page_content='function of the'), Document(metadata={'source': 'docs/attention.pdf', 'page': 3}, page_content='query with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention'), Document(metadata={'source': 'docs/attention.pdf', 'page': 3}, page_content='We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of'), Document(metadata={'source': 'docs/attention.pdf', 'page': 3}, page_content='queries and keys of dimension dk, and values of dimension dv. We compute the dot products of the'), Document(metadata={'source': 'docs/attention.pdf', 'page': 3}, page_content='query with all keys, divide each by√dk, and apply a softmax function to obtain the weights on the'), Document(metadata={'source': 'docs/attention.pdf', 'page': 3}, page_content='values.'), Document(metadata={'source': 'docs/attention.pdf', 'page': 3}, page_content='In practice, we compute the attention function on a set of queries simultaneously, packed together'), Document(metadata={'source': 'docs/attention.pdf', 'page': 3}, page_content='into a matrix Q. The keys and values are also packed together into matrices KandV. We compute'), Document(metadata={'source': 'docs/attention.pdf', 'page': 3}, page_content='the matrix of outputs as:\\nAttention( Q, K, V ) = softmax(QKT\\n√dk)V (1)'), Document(metadata={'source': 'docs/attention.pdf', 'page': 3}, page_content='The two most commonly used attention functions are additive attention [ 2], and dot-product (multi-'), Document(metadata={'source': 'docs/attention.pdf', 'page': 3}, page_content='plicative) attention. Dot-product attention is identical to our algorithm, except for the scaling'), Document(metadata={'source': 'docs/attention.pdf', 'page': 3}, page_content='for the scaling factor'), Document(metadata={'source': 'docs/attention.pdf', 'page': 3}, page_content='of1√dk. Additive attention computes the compatibility function using a feed-forward network with'), Document(metadata={'source': 'docs/attention.pdf', 'page': 3}, page_content='a single hidden layer. While the two are similar in theoretical complexity, dot-product attention'), Document(metadata={'source': 'docs/attention.pdf', 'page': 3}, page_content='attention is'), Document(metadata={'source': 'docs/attention.pdf', 'page': 3}, page_content='much faster and more space-efficient in practice, since it can be implemented using highly'), Document(metadata={'source': 'docs/attention.pdf', 'page': 3}, page_content='using highly optimized'), Document(metadata={'source': 'docs/attention.pdf', 'page': 3}, page_content='matrix multiplication code.'), Document(metadata={'source': 'docs/attention.pdf', 'page': 3}, page_content='While for small values of dkthe two mechanisms perform similarly, additive attention outperforms'), Document(metadata={'source': 'docs/attention.pdf', 'page': 3}, page_content='dot product attention without scaling for larger values of dk[3]. We suspect that for large values'), Document(metadata={'source': 'docs/attention.pdf', 'page': 3}, page_content='for large values of'), Document(metadata={'source': 'docs/attention.pdf', 'page': 3}, page_content='dk, the dot products grow large in magnitude, pushing the softmax function into regions where it'), Document(metadata={'source': 'docs/attention.pdf', 'page': 3}, page_content='regions where it has'), Document(metadata={'source': 'docs/attention.pdf', 'page': 3}, page_content='extremely small gradients4. To counteract this effect, we scale the dot products by1√dk.'), Document(metadata={'source': 'docs/attention.pdf', 'page': 3}, page_content='3.2.2 Multi-Head Attention'), Document(metadata={'source': 'docs/attention.pdf', 'page': 3}, page_content='Instead of performing a single attention function with dmodel-dimensional keys, values and queries,'), Document(metadata={'source': 'docs/attention.pdf', 'page': 3}, page_content='we found it beneficial to linearly project the queries, keys and values htimes with different,'), Document(metadata={'source': 'docs/attention.pdf', 'page': 3}, page_content='with different, learned'), Document(metadata={'source': 'docs/attention.pdf', 'page': 3}, page_content='linear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of'), Document(metadata={'source': 'docs/attention.pdf', 'page': 3}, page_content='queries, keys and values we then perform the attention function in parallel, yielding'), Document(metadata={'source': 'docs/attention.pdf', 'page': 3}, page_content='parallel, yielding dv-dimensional'), Document(metadata={'source': 'docs/attention.pdf', 'page': 3}, page_content='4To illustrate why the dot products get large, assume that the components of qandkare independent'), Document(metadata={'source': 'docs/attention.pdf', 'page': 3}, page_content='independent random'), Document(metadata={'source': 'docs/attention.pdf', 'page': 3}, page_content='variables with mean 0and variance 1. Then their dot product, q·k=Pdk'), Document(metadata={'source': 'docs/attention.pdf', 'page': 3}, page_content='i=1qiki, has mean 0and variance dk.\\n4'), Document(metadata={'source': 'docs/attention.pdf', 'page': 4}, page_content='output values. These are concatenated and once again projected, resulting in the final values, as'), Document(metadata={'source': 'docs/attention.pdf', 'page': 4}, page_content='depicted in Figure 2.'), Document(metadata={'source': 'docs/attention.pdf', 'page': 4}, page_content='Multi-head attention allows the model to jointly attend to information from different'), Document(metadata={'source': 'docs/attention.pdf', 'page': 4}, page_content='from different representation'), Document(metadata={'source': 'docs/attention.pdf', 'page': 4}, page_content='subspaces at different positions. With a single attention head, averaging inhibits this.'), Document(metadata={'source': 'docs/attention.pdf', 'page': 4}, page_content='MultiHead( Q, K, V ) = Concat(head 1, ...,head h)WO\\nwhere head i= Attention( QWQ\\ni, KWK\\ni, V WV\\ni)'), Document(metadata={'source': 'docs/attention.pdf', 'page': 4}, page_content='i, KWK\\ni, V WV\\ni)\\nWhere the projections are parameter matrices WQ\\ni∈Rdmodel×dk,WK\\ni∈Rdmodel×dk,WV'), Document(metadata={'source': 'docs/attention.pdf', 'page': 4}, page_content='i∈Rdmodel×dk,WV\\ni∈Rdmodel×dv\\nandWO∈Rhdv×dmodel.'), Document(metadata={'source': 'docs/attention.pdf', 'page': 4}, page_content='In this work we employ h= 8 parallel attention layers, or heads. For each of these we use'), Document(metadata={'source': 'docs/attention.pdf', 'page': 4}, page_content='dk=dv=dmodel/h= 64 . Due to the reduced dimension of each head, the total computational cost'), Document(metadata={'source': 'docs/attention.pdf', 'page': 4}, page_content='is similar to that of single-head attention with full dimensionality.'), Document(metadata={'source': 'docs/attention.pdf', 'page': 4}, page_content='3.2.3 Applications of Attention in our Model'), Document(metadata={'source': 'docs/attention.pdf', 'page': 4}, page_content='The Transformer uses multi-head attention in three different ways:'), Document(metadata={'source': 'docs/attention.pdf', 'page': 4}, page_content='•In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,'), Document(metadata={'source': 'docs/attention.pdf', 'page': 4}, page_content='and the memory keys and values come from the output of the encoder. This allows every'), Document(metadata={'source': 'docs/attention.pdf', 'page': 4}, page_content='position in the decoder to attend over all positions in the input sequence. This mimics the'), Document(metadata={'source': 'docs/attention.pdf', 'page': 4}, page_content='typical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].'), Document(metadata={'source': 'docs/attention.pdf', 'page': 4}, page_content='•The encoder contains self-attention layers. In a self-attention layer all of the keys, values'), Document(metadata={'source': 'docs/attention.pdf', 'page': 4}, page_content='and queries come from the same place, in this case, the output of the previous layer in the'), Document(metadata={'source': 'docs/attention.pdf', 'page': 4}, page_content='encoder. Each position in the encoder can attend to all positions in the previous layer of the'), Document(metadata={'source': 'docs/attention.pdf', 'page': 4}, page_content='encoder.'), Document(metadata={'source': 'docs/attention.pdf', 'page': 4}, page_content='•Similarly, self-attention layers in the decoder allow each position in the decoder to attend to'), Document(metadata={'source': 'docs/attention.pdf', 'page': 4}, page_content='all positions in the decoder up to and including that position. We need to prevent leftward'), Document(metadata={'source': 'docs/attention.pdf', 'page': 4}, page_content='information flow in the decoder to preserve the auto-regressive property. We implement this'), Document(metadata={'source': 'docs/attention.pdf', 'page': 4}, page_content='inside of scaled dot-product attention by masking out (setting to −∞) all values in the input'), Document(metadata={'source': 'docs/attention.pdf', 'page': 4}, page_content='of the softmax which correspond to illegal connections. See Figure 2.'), Document(metadata={'source': 'docs/attention.pdf', 'page': 4}, page_content='3.3 Position-wise Feed-Forward Networks'), Document(metadata={'source': 'docs/attention.pdf', 'page': 4}, page_content='In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully'), Document(metadata={'source': 'docs/attention.pdf', 'page': 4}, page_content='connected feed-forward network, which is applied to each position separately and identically. This'), Document(metadata={'source': 'docs/attention.pdf', 'page': 4}, page_content='consists of two linear transformations with a ReLU activation in between.'), Document(metadata={'source': 'docs/attention.pdf', 'page': 4}, page_content='FFN( x) = max(0 , xW 1+b1)W2+b2 (2)'), Document(metadata={'source': 'docs/attention.pdf', 'page': 4}, page_content='While the linear transformations are the same across different positions, they use different'), Document(metadata={'source': 'docs/attention.pdf', 'page': 4}, page_content='they use different parameters'), Document(metadata={'source': 'docs/attention.pdf', 'page': 4}, page_content='from layer to layer. Another way of describing this is as two convolutions with kernel size 1.'), Document(metadata={'source': 'docs/attention.pdf', 'page': 4}, page_content='The dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality'), Document(metadata={'source': 'docs/attention.pdf', 'page': 4}, page_content='dff= 2048 .\\n3.4 Embeddings and Softmax'), Document(metadata={'source': 'docs/attention.pdf', 'page': 4}, page_content='Similarly to other sequence transduction models, we use learned embeddings to convert the input'), Document(metadata={'source': 'docs/attention.pdf', 'page': 4}, page_content='tokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear'), Document(metadata={'source': 'docs/attention.pdf', 'page': 4}, page_content='learned linear transfor-'), Document(metadata={'source': 'docs/attention.pdf', 'page': 4}, page_content='mation and softmax function to convert the decoder output to predicted next-token probabilities. In'), Document(metadata={'source': 'docs/attention.pdf', 'page': 4}, page_content='our model, we share the same weight matrix between the two embedding layers and the pre-softmax'), Document(metadata={'source': 'docs/attention.pdf', 'page': 4}, page_content='linear transformation, similar to [ 30]. In the embedding layers, we multiply those weights'), Document(metadata={'source': 'docs/attention.pdf', 'page': 4}, page_content='those weights by√dmodel.'), Document(metadata={'source': 'docs/attention.pdf', 'page': 4}, page_content='5'), Document(metadata={'source': 'docs/attention.pdf', 'page': 5}, page_content='Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations'), Document(metadata={'source': 'docs/attention.pdf', 'page': 5}, page_content='for different layer types. nis the sequence length, dis the representation dimension, kis the'), Document(metadata={'source': 'docs/attention.pdf', 'page': 5}, page_content='dimension, kis the kernel'), Document(metadata={'source': 'docs/attention.pdf', 'page': 5}, page_content='size of convolutions and rthe size of the neighborhood in restricted self-attention.'), Document(metadata={'source': 'docs/attention.pdf', 'page': 5}, page_content='Layer Type Complexity per Layer Sequential Maximum Path Length\\nOperations'), Document(metadata={'source': 'docs/attention.pdf', 'page': 5}, page_content='Operations\\nSelf-Attention O(n2·d) O(1) O(1)\\nRecurrent O(n·d2) O(n) O(n)'), Document(metadata={'source': 'docs/attention.pdf', 'page': 5}, page_content='Convolutional O(k·n·d2) O(1) O(logk(n))\\nSelf-Attention (restricted) O(r·n·d) O(1) O(n/r)'), Document(metadata={'source': 'docs/attention.pdf', 'page': 5}, page_content='3.5 Positional Encoding'), Document(metadata={'source': 'docs/attention.pdf', 'page': 5}, page_content='Since our model contains no recurrence and no convolution, in order for the model to make use of'), Document(metadata={'source': 'docs/attention.pdf', 'page': 5}, page_content='to make use of the'), Document(metadata={'source': 'docs/attention.pdf', 'page': 5}, page_content='order of the sequence, we must inject some information about the relative or absolute position of'), Document(metadata={'source': 'docs/attention.pdf', 'page': 5}, page_content='position of the'), Document(metadata={'source': 'docs/attention.pdf', 'page': 5}, page_content='tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the'), Document(metadata={'source': 'docs/attention.pdf', 'page': 5}, page_content='bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel'), Document(metadata={'source': 'docs/attention.pdf', 'page': 5}, page_content='as the embeddings, so that the two can be summed. There are many choices of positional encodings,'), Document(metadata={'source': 'docs/attention.pdf', 'page': 5}, page_content='learned and fixed [9].\\nIn this work, we use sine and cosine functions of different frequencies:'), Document(metadata={'source': 'docs/attention.pdf', 'page': 5}, page_content='PE(pos,2i)=sin(pos/100002i/d model)\\nPE(pos,2i+1)=cos(pos/100002i/d model)'), Document(metadata={'source': 'docs/attention.pdf', 'page': 5}, page_content='where posis the position and iis the dimension. That is, each dimension of the positional encoding'), Document(metadata={'source': 'docs/attention.pdf', 'page': 5}, page_content='corresponds to a sinusoid. The wavelengths form a geometric progression from 2πto10000 ·2π. We'), Document(metadata={'source': 'docs/attention.pdf', 'page': 5}, page_content='chose this function because we hypothesized it would allow the model to easily learn to attend by'), Document(metadata={'source': 'docs/attention.pdf', 'page': 5}, page_content='relative positions, since for any fixed offset k,PEpos+kcan be represented as a linear function of'), Document(metadata={'source': 'docs/attention.pdf', 'page': 5}, page_content='PEpos.'), Document(metadata={'source': 'docs/attention.pdf', 'page': 5}, page_content='We also experimented with using learned positional embeddings [ 9] instead, and found that the two'), Document(metadata={'source': 'docs/attention.pdf', 'page': 5}, page_content='versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version'), Document(metadata={'source': 'docs/attention.pdf', 'page': 5}, page_content='because it may allow the model to extrapolate to sequence lengths longer than the ones encountered'), Document(metadata={'source': 'docs/attention.pdf', 'page': 5}, page_content='during training.\\n4 Why Self-Attention'), Document(metadata={'source': 'docs/attention.pdf', 'page': 5}, page_content='In this section we compare various aspects of self-attention layers to the recurrent and convolu-'), Document(metadata={'source': 'docs/attention.pdf', 'page': 5}, page_content='tional layers commonly used for mapping one variable-length sequence of symbol representations'), Document(metadata={'source': 'docs/attention.pdf', 'page': 5}, page_content='(x1, ..., x n)to another sequence of equal length (z1, ..., z n), with xi, zi∈Rd, such as a hidden'), Document(metadata={'source': 'docs/attention.pdf', 'page': 5}, page_content='layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention'), Document(metadata={'source': 'docs/attention.pdf', 'page': 5}, page_content='of self-attention we'), Document(metadata={'source': 'docs/attention.pdf', 'page': 5}, page_content='consider three desiderata.'), Document(metadata={'source': 'docs/attention.pdf', 'page': 5}, page_content='One is the total computational complexity per layer. Another is the amount of computation that can'), Document(metadata={'source': 'docs/attention.pdf', 'page': 5}, page_content='be parallelized, as measured by the minimum number of sequential operations required.'), Document(metadata={'source': 'docs/attention.pdf', 'page': 5}, page_content='The third is the path length between long-range dependencies in the network. Learning long-range'), Document(metadata={'source': 'docs/attention.pdf', 'page': 5}, page_content='dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the'), Document(metadata={'source': 'docs/attention.pdf', 'page': 5}, page_content='ability to learn such dependencies is the length of the paths forward and backward signals have to'), Document(metadata={'source': 'docs/attention.pdf', 'page': 5}, page_content='traverse in the network. The shorter these paths between any combination of positions in the input'), Document(metadata={'source': 'docs/attention.pdf', 'page': 5}, page_content='and output sequences, the easier it is to learn long-range dependencies [ 12]. Hence we also'), Document(metadata={'source': 'docs/attention.pdf', 'page': 5}, page_content='12]. Hence we also compare'), Document(metadata={'source': 'docs/attention.pdf', 'page': 5}, page_content='the maximum path length between any two input and output positions in networks composed of the'), Document(metadata={'source': 'docs/attention.pdf', 'page': 5}, page_content='different layer types.'), Document(metadata={'source': 'docs/attention.pdf', 'page': 5}, page_content='As noted in Table 1, a self-attention layer connects all positions with a constant number of'), Document(metadata={'source': 'docs/attention.pdf', 'page': 5}, page_content='constant number of sequentially'), Document(metadata={'source': 'docs/attention.pdf', 'page': 5}, page_content='executed operations, whereas a recurrent layer requires O(n)sequential operations. In terms of'), Document(metadata={'source': 'docs/attention.pdf', 'page': 5}, page_content='computational complexity, self-attention layers are faster than recurrent layers when the sequence'), Document(metadata={'source': 'docs/attention.pdf', 'page': 5}, page_content='6'), Document(metadata={'source': 'docs/attention.pdf', 'page': 6}, page_content='length nis smaller than the representation dimensionality d, which is most often the case with'), Document(metadata={'source': 'docs/attention.pdf', 'page': 6}, page_content='sentence representations used by state-of-the-art models in machine translations, such as'), Document(metadata={'source': 'docs/attention.pdf', 'page': 6}, page_content='such as word-piece'), Document(metadata={'source': 'docs/attention.pdf', 'page': 6}, page_content='[38] and byte-pair [ 31] representations. To improve computational performance for tasks involving'), Document(metadata={'source': 'docs/attention.pdf', 'page': 6}, page_content='very long sequences, self-attention could be restricted to considering only a neighborhood of size'), Document(metadata={'source': 'docs/attention.pdf', 'page': 6}, page_content='of size rin'), Document(metadata={'source': 'docs/attention.pdf', 'page': 6}, page_content='the input sequence centered around the respective output position. This would increase the maximum'), Document(metadata={'source': 'docs/attention.pdf', 'page': 6}, page_content='path length to O(n/r). We plan to investigate this approach further in future work.'), Document(metadata={'source': 'docs/attention.pdf', 'page': 6}, page_content='A single convolutional layer with kernel width k < n does not connect all pairs of input and output'), Document(metadata={'source': 'docs/attention.pdf', 'page': 6}, page_content='positions. Doing so requires a stack of O(n/k)convolutional layers in the case of contiguous'), Document(metadata={'source': 'docs/attention.pdf', 'page': 6}, page_content='case of contiguous kernels,'), Document(metadata={'source': 'docs/attention.pdf', 'page': 6}, page_content='orO(logk(n))in the case of dilated convolutions [ 18], increasing the length of the longest paths'), Document(metadata={'source': 'docs/attention.pdf', 'page': 6}, page_content='between any two positions in the network. Convolutional layers are generally more expensive than'), Document(metadata={'source': 'docs/attention.pdf', 'page': 6}, page_content='recurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity'), Document(metadata={'source': 'docs/attention.pdf', 'page': 6}, page_content='considerably, to O(k·n·d+n·d2). Even with k=n, however, the complexity of a separable'), Document(metadata={'source': 'docs/attention.pdf', 'page': 6}, page_content='convolution is equal to the combination of a self-attention layer and a point-wise feed-forward'), Document(metadata={'source': 'docs/attention.pdf', 'page': 6}, page_content='feed-forward layer,'), Document(metadata={'source': 'docs/attention.pdf', 'page': 6}, page_content='the approach we take in our model.'), Document(metadata={'source': 'docs/attention.pdf', 'page': 6}, page_content='As side benefit, self-attention could yield more interpretable models. We inspect attention'), Document(metadata={'source': 'docs/attention.pdf', 'page': 6}, page_content='inspect attention distributions'), Document(metadata={'source': 'docs/attention.pdf', 'page': 6}, page_content='from our models and present and discuss examples in the appendix. Not only do individual attention'), Document(metadata={'source': 'docs/attention.pdf', 'page': 6}, page_content='heads clearly learn to perform different tasks, many appear to exhibit behavior related to the'), Document(metadata={'source': 'docs/attention.pdf', 'page': 6}, page_content='related to the syntactic'), Document(metadata={'source': 'docs/attention.pdf', 'page': 6}, page_content='and semantic structure of the sentences.\\n5 Training'), Document(metadata={'source': 'docs/attention.pdf', 'page': 6}, page_content='5 Training\\nThis section describes the training regime for our models.'), Document(metadata={'source': 'docs/attention.pdf', 'page': 6}, page_content='5.1 Training Data and Batching'), Document(metadata={'source': 'docs/attention.pdf', 'page': 6}, page_content='We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million'), Document(metadata={'source': 'docs/attention.pdf', 'page': 6}, page_content='sentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-'), Document(metadata={'source': 'docs/attention.pdf', 'page': 6}, page_content='target vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT'), Document(metadata={'source': 'docs/attention.pdf', 'page': 6}, page_content='2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece'), Document(metadata={'source': 'docs/attention.pdf', 'page': 6}, page_content='vocabulary [ 38]. Sentence pairs were batched together by approximate sequence length. Each'), Document(metadata={'source': 'docs/attention.pdf', 'page': 6}, page_content='length. Each training'), Document(metadata={'source': 'docs/attention.pdf', 'page': 6}, page_content='batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000'), Document(metadata={'source': 'docs/attention.pdf', 'page': 6}, page_content='target tokens.\\n5.2 Hardware and Schedule'), Document(metadata={'source': 'docs/attention.pdf', 'page': 6}, page_content='We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using'), Document(metadata={'source': 'docs/attention.pdf', 'page': 6}, page_content='the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We'), Document(metadata={'source': 'docs/attention.pdf', 'page': 6}, page_content='trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on'), Document(metadata={'source': 'docs/attention.pdf', 'page': 6}, page_content='on the'), Document(metadata={'source': 'docs/attention.pdf', 'page': 6}, page_content='bottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps'), Document(metadata={'source': 'docs/attention.pdf', 'page': 6}, page_content='(3.5 days).\\n5.3 Optimizer'), Document(metadata={'source': 'docs/attention.pdf', 'page': 6}, page_content='We used the Adam optimizer [ 20] with β1= 0.9,β2= 0.98andϵ= 10−9. We varied the learning'), Document(metadata={'source': 'docs/attention.pdf', 'page': 6}, page_content='rate over the course of training, according to the formula:\\nlrate =d−0.5'), Document(metadata={'source': 'docs/attention.pdf', 'page': 6}, page_content='lrate =d−0.5\\nmodel·min(step_num−0.5, step _num·warmup _steps−1.5) (3)'), Document(metadata={'source': 'docs/attention.pdf', 'page': 6}, page_content='This corresponds to increasing the learning rate linearly for the first warmup _steps training'), Document(metadata={'source': 'docs/attention.pdf', 'page': 6}, page_content='_steps training steps,'), Document(metadata={'source': 'docs/attention.pdf', 'page': 6}, page_content='and decreasing it thereafter proportionally to the inverse square root of the step number. We used'), Document(metadata={'source': 'docs/attention.pdf', 'page': 6}, page_content='warmup _steps = 4000 .\\n5.4 Regularization\\nWe employ three types of regularization during training:'), Document(metadata={'source': 'docs/attention.pdf', 'page': 6}, page_content='7'), Document(metadata={'source': 'docs/attention.pdf', 'page': 7}, page_content='Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the'), Document(metadata={'source': 'docs/attention.pdf', 'page': 7}, page_content='English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.'), Document(metadata={'source': 'docs/attention.pdf', 'page': 7}, page_content='ModelBLEU Training Cost (FLOPs)\\nEN-DE EN-FR EN-DE EN-FR\\nByteNet [18] 23.75'), Document(metadata={'source': 'docs/attention.pdf', 'page': 7}, page_content='ByteNet [18] 23.75\\nDeep-Att + PosUnk [39] 39.2 1.0·1020\\nGNMT + RL [38] 24.6 39.92 2.3·10191.4·1020'), Document(metadata={'source': 'docs/attention.pdf', 'page': 7}, page_content='ConvS2S [9] 25.16 40.46 9.6·10181.5·1020\\nMoE [32] 26.03 40.56 2.0·10191.2·1020'), Document(metadata={'source': 'docs/attention.pdf', 'page': 7}, page_content='Deep-Att + PosUnk Ensemble [39] 40.4 8.0·1020\\nGNMT + RL Ensemble [38] 26.30 41.16 1.8·10201.1·1021'), Document(metadata={'source': 'docs/attention.pdf', 'page': 7}, page_content='ConvS2S Ensemble [9] 26.36 41.29 7.7·10191.2·1021\\nTransformer (base model) 27.3 38.1 3.3·1018'), Document(metadata={'source': 'docs/attention.pdf', 'page': 7}, page_content='Transformer (big) 28.4 41.8 2.3·1019'), Document(metadata={'source': 'docs/attention.pdf', 'page': 7}, page_content='Residual Dropout We apply dropout [ 33] to the output of each sub-layer, before it is added to the'), Document(metadata={'source': 'docs/attention.pdf', 'page': 7}, page_content='sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the'), Document(metadata={'source': 'docs/attention.pdf', 'page': 7}, page_content='positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of'), Document(metadata={'source': 'docs/attention.pdf', 'page': 7}, page_content='Pdrop= 0.1.'), Document(metadata={'source': 'docs/attention.pdf', 'page': 7}, page_content='Label Smoothing During training, we employed label smoothing of value ϵls= 0.1[36]. This'), Document(metadata={'source': 'docs/attention.pdf', 'page': 7}, page_content='hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.'), Document(metadata={'source': 'docs/attention.pdf', 'page': 7}, page_content='6 Results\\n6.1 Machine Translation'), Document(metadata={'source': 'docs/attention.pdf', 'page': 7}, page_content='On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)'), Document(metadata={'source': 'docs/attention.pdf', 'page': 7}, page_content='in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0'), Document(metadata={'source': 'docs/attention.pdf', 'page': 7}, page_content='BLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is'), Document(metadata={'source': 'docs/attention.pdf', 'page': 7}, page_content='listed in the bottom line of Table 3. Training took 3.5days on 8P100 GPUs. Even our base model'), Document(metadata={'source': 'docs/attention.pdf', 'page': 7}, page_content='surpasses all previously published models and ensembles, at a fraction of the training cost of any'), Document(metadata={'source': 'docs/attention.pdf', 'page': 7}, page_content='cost of any of'), Document(metadata={'source': 'docs/attention.pdf', 'page': 7}, page_content='the competitive models.'), Document(metadata={'source': 'docs/attention.pdf', 'page': 7}, page_content='On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,'), Document(metadata={'source': 'docs/attention.pdf', 'page': 7}, page_content='outperforming all of the previously published single models, at less than 1/4the training cost of'), Document(metadata={'source': 'docs/attention.pdf', 'page': 7}, page_content='training cost of the'), Document(metadata={'source': 'docs/attention.pdf', 'page': 7}, page_content='previous state-of-the-art model. The Transformer (big) model trained for English-to-French used'), Document(metadata={'source': 'docs/attention.pdf', 'page': 7}, page_content='dropout rate Pdrop= 0.1, instead of 0.3.'), Document(metadata={'source': 'docs/attention.pdf', 'page': 7}, page_content='For the base models, we used a single model obtained by averaging the last 5 checkpoints, which'), Document(metadata={'source': 'docs/attention.pdf', 'page': 7}, page_content='were written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We'), Document(metadata={'source': 'docs/attention.pdf', 'page': 7}, page_content='used beam search with a beam size of 4and length penalty α= 0.6[38]. These hyperparameters'), Document(metadata={'source': 'docs/attention.pdf', 'page': 7}, page_content='were chosen after experimentation on the development set. We set the maximum output length during'), Document(metadata={'source': 'docs/attention.pdf', 'page': 7}, page_content='inference to input length + 50, but terminate early when possible [38].'), Document(metadata={'source': 'docs/attention.pdf', 'page': 7}, page_content='Table 2 summarizes our results and compares our translation quality and training costs to other'), Document(metadata={'source': 'docs/attention.pdf', 'page': 7}, page_content='costs to other model'), Document(metadata={'source': 'docs/attention.pdf', 'page': 7}, page_content='architectures from the literature. We estimate the number of floating point operations used to'), Document(metadata={'source': 'docs/attention.pdf', 'page': 7}, page_content='operations used to train a'), Document(metadata={'source': 'docs/attention.pdf', 'page': 7}, page_content='model by multiplying the training time, the number of GPUs used, and an estimate of the sustained'), Document(metadata={'source': 'docs/attention.pdf', 'page': 7}, page_content='single-precision floating-point capacity of each GPU5.\\n6.2 Model Variations'), Document(metadata={'source': 'docs/attention.pdf', 'page': 7}, page_content='To evaluate the importance of different components of the Transformer, we varied our base model'), Document(metadata={'source': 'docs/attention.pdf', 'page': 7}, page_content='in different ways, measuring the change in performance on English-to-German translation on the'), Document(metadata={'source': 'docs/attention.pdf', 'page': 7}, page_content='5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8'), Document(metadata={'source': 'docs/attention.pdf', 'page': 8}, page_content='Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the'), Document(metadata={'source': 'docs/attention.pdf', 'page': 8}, page_content='to those of the base'), Document(metadata={'source': 'docs/attention.pdf', 'page': 8}, page_content='model. All metrics are on the English-to-German translation development set, newstest2013. Listed'), Document(metadata={'source': 'docs/attention.pdf', 'page': 8}, page_content='perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to'), Document(metadata={'source': 'docs/attention.pdf', 'page': 8}, page_content='per-word perplexities.\\nN d model dff h d k dvPdrop ϵlstrain PPL BLEU params\\nsteps (dev) (dev) ×106'), Document(metadata={'source': 'docs/attention.pdf', 'page': 8}, page_content='base 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)1 512 512 5.29 24.9\\n4 128 128 5.00 25.5'), Document(metadata={'source': 'docs/attention.pdf', 'page': 8}, page_content='4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B)16 5.16 25.1 58\\n32 5.01 25.4 60'), Document(metadata={'source': 'docs/attention.pdf', 'page': 8}, page_content='32 5.01 25.4 60\\n(C)2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28'), Document(metadata={'source': 'docs/attention.pdf', 'page': 8}, page_content='1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)0.0 5.77 24.6\\n0.2 4.95 25.5'), Document(metadata={'source': 'docs/attention.pdf', 'page': 8}, page_content='0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7'), Document(metadata={'source': 'docs/attention.pdf', 'page': 8}, page_content='big 6 1024 4096 16 0.3 300K 4.33 26.4 213'), Document(metadata={'source': 'docs/attention.pdf', 'page': 8}, page_content='development set, newstest2013. We used beam search as described in the previous section, but no'), Document(metadata={'source': 'docs/attention.pdf', 'page': 8}, page_content='checkpoint averaging. We present these results in Table 3.'), Document(metadata={'source': 'docs/attention.pdf', 'page': 8}, page_content='In Table 3 rows (A), we vary the number of attention heads and the attention key and value'), Document(metadata={'source': 'docs/attention.pdf', 'page': 8}, page_content='key and value dimensions,'), Document(metadata={'source': 'docs/attention.pdf', 'page': 8}, page_content='keeping the amount of computation constant, as described in Section 3.2.2. While single-head'), Document(metadata={'source': 'docs/attention.pdf', 'page': 8}, page_content='attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.'), Document(metadata={'source': 'docs/attention.pdf', 'page': 8}, page_content='In Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This'), Document(metadata={'source': 'docs/attention.pdf', 'page': 8}, page_content='suggests that determining compatibility is not easy and that a more sophisticated compatibility'), Document(metadata={'source': 'docs/attention.pdf', 'page': 8}, page_content='function than dot product may be beneficial. We further observe in rows (C) and (D) that, as'), Document(metadata={'source': 'docs/attention.pdf', 'page': 8}, page_content='and (D) that, as expected,'), Document(metadata={'source': 'docs/attention.pdf', 'page': 8}, page_content='bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we'), Document(metadata={'source': 'docs/attention.pdf', 'page': 8}, page_content='In row (E) we replace our'), Document(metadata={'source': 'docs/attention.pdf', 'page': 8}, page_content='sinusoidal positional encoding with learned positional embeddings [ 9], and observe nearly'), Document(metadata={'source': 'docs/attention.pdf', 'page': 8}, page_content='and observe nearly identical'), Document(metadata={'source': 'docs/attention.pdf', 'page': 8}, page_content='results to the base model.\\n6.3 English Constituency Parsing'), Document(metadata={'source': 'docs/attention.pdf', 'page': 8}, page_content='To evaluate if the Transformer can generalize to other tasks we performed experiments on English'), Document(metadata={'source': 'docs/attention.pdf', 'page': 8}, page_content='constituency parsing. This task presents specific challenges: the output is subject to strong'), Document(metadata={'source': 'docs/attention.pdf', 'page': 8}, page_content='subject to strong structural'), Document(metadata={'source': 'docs/attention.pdf', 'page': 8}, page_content='constraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence'), Document(metadata={'source': 'docs/attention.pdf', 'page': 8}, page_content='models have not been able to attain state-of-the-art results in small-data regimes [37].'), Document(metadata={'source': 'docs/attention.pdf', 'page': 8}, page_content='We trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the'), Document(metadata={'source': 'docs/attention.pdf', 'page': 8}, page_content='Penn Treebank [ 25], about 40K training sentences. We also trained it in a semi-supervised setting,'), Document(metadata={'source': 'docs/attention.pdf', 'page': 8}, page_content='using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences'), Document(metadata={'source': 'docs/attention.pdf', 'page': 8}, page_content='[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens'), Document(metadata={'source': 'docs/attention.pdf', 'page': 8}, page_content='for the semi-supervised setting.'), Document(metadata={'source': 'docs/attention.pdf', 'page': 8}, page_content='We performed only a small number of experiments to select the dropout, both attention and residual'), Document(metadata={'source': 'docs/attention.pdf', 'page': 8}, page_content='(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters'), Document(metadata={'source': 'docs/attention.pdf', 'page': 8}, page_content='remained unchanged from the English-to-German base translation model. During inference, we\\n9'), Document(metadata={'source': 'docs/attention.pdf', 'page': 9}, page_content='Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23'), Document(metadata={'source': 'docs/attention.pdf', 'page': 9}, page_content='of WSJ)\\nParser Training WSJ 23 F1\\nVinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3'), Document(metadata={'source': 'docs/attention.pdf', 'page': 9}, page_content='Petrov et al. (2006) [29] WSJ only, discriminative 90.4'), Document(metadata={'source': 'docs/attention.pdf', 'page': 9}, page_content='Zhu et al. (2013) [40] WSJ only, discriminative 90.4'), Document(metadata={'source': 'docs/attention.pdf', 'page': 9}, page_content='Dyer et al. (2016) [8] WSJ only, discriminative 91.7'), Document(metadata={'source': 'docs/attention.pdf', 'page': 9}, page_content='Transformer (4 layers) WSJ only, discriminative 91.3\\nZhu et al. (2013) [40] semi-supervised 91.3'), Document(metadata={'source': 'docs/attention.pdf', 'page': 9}, page_content='Huang & Harper (2009) [14] semi-supervised 91.3\\nMcClosky et al. (2006) [26] semi-supervised 92.1'), Document(metadata={'source': 'docs/attention.pdf', 'page': 9}, page_content='Vinyals & Kaiser el al. (2014) [37] semi-supervised 92.1'), Document(metadata={'source': 'docs/attention.pdf', 'page': 9}, page_content='Transformer (4 layers) semi-supervised 92.7\\nLuong et al. (2015) [23] multi-task 93.0'), Document(metadata={'source': 'docs/attention.pdf', 'page': 9}, page_content='Dyer et al. (2016) [8] generative 93.3'), Document(metadata={'source': 'docs/attention.pdf', 'page': 9}, page_content='increased the maximum output length to input length + 300. We used a beam size of 21andα= 0.3'), Document(metadata={'source': 'docs/attention.pdf', 'page': 9}, page_content='for both WSJ only and the semi-supervised setting.'), Document(metadata={'source': 'docs/attention.pdf', 'page': 9}, page_content='Our results in Table 4 show that despite the lack of task-specific tuning our model performs sur-'), Document(metadata={'source': 'docs/attention.pdf', 'page': 9}, page_content='prisingly well, yielding better results than all previously reported models with the exception of'), Document(metadata={'source': 'docs/attention.pdf', 'page': 9}, page_content='the exception of the'), Document(metadata={'source': 'docs/attention.pdf', 'page': 9}, page_content='Recurrent Neural Network Grammar [8].'), Document(metadata={'source': 'docs/attention.pdf', 'page': 9}, page_content='In contrast to RNN sequence-to-sequence models [ 37], the Transformer outperforms the Berkeley-'), Document(metadata={'source': 'docs/attention.pdf', 'page': 9}, page_content='Parser [29] even when training only on the WSJ training set of 40K sentences.\\n7 Conclusion'), Document(metadata={'source': 'docs/attention.pdf', 'page': 9}, page_content='In this work, we presented the Transformer, the first sequence transduction model based entirely on'), Document(metadata={'source': 'docs/attention.pdf', 'page': 9}, page_content='attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with'), Document(metadata={'source': 'docs/attention.pdf', 'page': 9}, page_content='multi-headed self-attention.'), Document(metadata={'source': 'docs/attention.pdf', 'page': 9}, page_content='For translation tasks, the Transformer can be trained significantly faster than architectures based'), Document(metadata={'source': 'docs/attention.pdf', 'page': 9}, page_content='on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014'), Document(metadata={'source': 'docs/attention.pdf', 'page': 9}, page_content='English-to-French translation tasks, we achieve a new state of the art. In the former task our best'), Document(metadata={'source': 'docs/attention.pdf', 'page': 9}, page_content='model outperforms even all previously reported ensembles.'), Document(metadata={'source': 'docs/attention.pdf', 'page': 9}, page_content='We are excited about the future of attention-based models and plan to apply them to other tasks. We'), Document(metadata={'source': 'docs/attention.pdf', 'page': 9}, page_content='plan to extend the Transformer to problems involving input and output modalities other than text'), Document(metadata={'source': 'docs/attention.pdf', 'page': 9}, page_content='other than text and'), Document(metadata={'source': 'docs/attention.pdf', 'page': 9}, page_content='to investigate local, restricted attention mechanisms to efficiently handle large inputs and'), Document(metadata={'source': 'docs/attention.pdf', 'page': 9}, page_content='large inputs and outputs'), Document(metadata={'source': 'docs/attention.pdf', 'page': 9}, page_content='such as images, audio and video. Making generation less sequential is another research goals of'), Document(metadata={'source': 'docs/attention.pdf', 'page': 9}, page_content='research goals of ours.'), Document(metadata={'source': 'docs/attention.pdf', 'page': 9}, page_content='The code we used to train and evaluate our models is available at https://github.com/'), Document(metadata={'source': 'docs/attention.pdf', 'page': 9}, page_content='tensorflow/tensor2tensor .'), Document(metadata={'source': 'docs/attention.pdf', 'page': 9}, page_content='Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful'), Document(metadata={'source': 'docs/attention.pdf', 'page': 9}, page_content='comments, corrections and inspiration.\\nReferences'), Document(metadata={'source': 'docs/attention.pdf', 'page': 9}, page_content='[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint'), Document(metadata={'source': 'docs/attention.pdf', 'page': 9}, page_content='arXiv:1607.06450 , 2016.'), Document(metadata={'source': 'docs/attention.pdf', 'page': 9}, page_content='[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly'), Document(metadata={'source': 'docs/attention.pdf', 'page': 9}, page_content='learning to align and translate. CoRR , abs/1409.0473, 2014.'), Document(metadata={'source': 'docs/attention.pdf', 'page': 9}, page_content='[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural'), Document(metadata={'source': 'docs/attention.pdf', 'page': 9}, page_content='machine translation architectures. CoRR , abs/1703.03906, 2017.'), Document(metadata={'source': 'docs/attention.pdf', 'page': 9}, page_content='[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine'), Document(metadata={'source': 'docs/attention.pdf', 'page': 9}, page_content='reading. arXiv preprint arXiv:1601.06733 , 2016.\\n10'), Document(metadata={'source': 'docs/attention.pdf', 'page': 10}, page_content='[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,'), Document(metadata={'source': 'docs/attention.pdf', 'page': 10}, page_content='and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical'), Document(metadata={'source': 'docs/attention.pdf', 'page': 10}, page_content='machine translation. CoRR , abs/1406.1078, 2014.'), Document(metadata={'source': 'docs/attention.pdf', 'page': 10}, page_content='[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv'), Document(metadata={'source': 'docs/attention.pdf', 'page': 10}, page_content='preprint arXiv:1610.02357 , 2016.'), Document(metadata={'source': 'docs/attention.pdf', 'page': 10}, page_content='[7]Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation'), Document(metadata={'source': 'docs/attention.pdf', 'page': 10}, page_content='of gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.'), Document(metadata={'source': 'docs/attention.pdf', 'page': 10}, page_content='[8]Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural'), Document(metadata={'source': 'docs/attention.pdf', 'page': 10}, page_content='network grammars. In Proc. of NAACL , 2016.'), Document(metadata={'source': 'docs/attention.pdf', 'page': 10}, page_content='[9]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-'), Document(metadata={'source': 'docs/attention.pdf', 'page': 10}, page_content='tional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.'), Document(metadata={'source': 'docs/attention.pdf', 'page': 10}, page_content='[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint'), Document(metadata={'source': 'docs/attention.pdf', 'page': 10}, page_content='arXiv:1308.0850 , 2013.'), Document(metadata={'source': 'docs/attention.pdf', 'page': 10}, page_content='[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-'), Document(metadata={'source': 'docs/attention.pdf', 'page': 10}, page_content='age recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern'), Document(metadata={'source': 'docs/attention.pdf', 'page': 10}, page_content='Recognition , pages 770–778, 2016.'), Document(metadata={'source': 'docs/attention.pdf', 'page': 10}, page_content='[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in'), Document(metadata={'source': 'docs/attention.pdf', 'page': 10}, page_content='recurrent nets: the difficulty of learning long-term dependencies, 2001.'), Document(metadata={'source': 'docs/attention.pdf', 'page': 10}, page_content='[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation ,'), Document(metadata={'source': 'docs/attention.pdf', 'page': 10}, page_content='9(8):1735–1780, 1997.'), Document(metadata={'source': 'docs/attention.pdf', 'page': 10}, page_content='[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations'), Document(metadata={'source': 'docs/attention.pdf', 'page': 10}, page_content='across languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural'), Document(metadata={'source': 'docs/attention.pdf', 'page': 10}, page_content='Language Processing , pages 832–841. ACL, August 2009.'), Document(metadata={'source': 'docs/attention.pdf', 'page': 10}, page_content='[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring'), Document(metadata={'source': 'docs/attention.pdf', 'page': 10}, page_content='the limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.'), Document(metadata={'source': 'docs/attention.pdf', 'page': 10}, page_content='[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural'), Document(metadata={'source': 'docs/attention.pdf', 'page': 10}, page_content='Information Processing Systems, (NIPS) , 2016.'), Document(metadata={'source': 'docs/attention.pdf', 'page': 10}, page_content='[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference'), Document(metadata={'source': 'docs/attention.pdf', 'page': 10}, page_content='on Learning Representations (ICLR) , 2016.'), Document(metadata={'source': 'docs/attention.pdf', 'page': 10}, page_content='[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-'), Document(metadata={'source': 'docs/attention.pdf', 'page': 10}, page_content='ray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,'), Document(metadata={'source': 'docs/attention.pdf', 'page': 10}, page_content='2017.'), Document(metadata={'source': 'docs/attention.pdf', 'page': 10}, page_content='[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.'), Document(metadata={'source': 'docs/attention.pdf', 'page': 10}, page_content='InInternational Conference on Learning Representations , 2017.'), Document(metadata={'source': 'docs/attention.pdf', 'page': 10}, page_content='[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.'), Document(metadata={'source': 'docs/attention.pdf', 'page': 10}, page_content='[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint'), Document(metadata={'source': 'docs/attention.pdf', 'page': 10}, page_content='arXiv:1703.10722 , 2017.'), Document(metadata={'source': 'docs/attention.pdf', 'page': 10}, page_content='[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen'), Document(metadata={'source': 'docs/attention.pdf', 'page': 10}, page_content='Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint'), Document(metadata={'source': 'docs/attention.pdf', 'page': 10}, page_content='arXiv:1703.03130 , 2017.'), Document(metadata={'source': 'docs/attention.pdf', 'page': 10}, page_content='[23] Minh-Thang Luong, Quoc V . Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task'), Document(metadata={'source': 'docs/attention.pdf', 'page': 10}, page_content='sequence to sequence learning. arXiv preprint arXiv:1511.06114 , 2015.'), Document(metadata={'source': 'docs/attention.pdf', 'page': 10}, page_content='[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-'), Document(metadata={'source': 'docs/attention.pdf', 'page': 10}, page_content='based neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\n11'), Document(metadata={'source': 'docs/attention.pdf', 'page': 11}, page_content='[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated'), Document(metadata={'source': 'docs/attention.pdf', 'page': 11}, page_content='corpus of english: The penn treebank. Computational linguistics , 19(2):313–330, 1993.'), Document(metadata={'source': 'docs/attention.pdf', 'page': 11}, page_content='[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In'), Document(metadata={'source': 'docs/attention.pdf', 'page': 11}, page_content='Proceedings of the Human Language Technology Conference of the NAACL, Main Conference ,'), Document(metadata={'source': 'docs/attention.pdf', 'page': 11}, page_content='pages 152–159. ACL, June 2006.'), Document(metadata={'source': 'docs/attention.pdf', 'page': 11}, page_content='[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention'), Document(metadata={'source': 'docs/attention.pdf', 'page': 11}, page_content='model. In Empirical Methods in Natural Language Processing , 2016.'), Document(metadata={'source': 'docs/attention.pdf', 'page': 11}, page_content='[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive'), Document(metadata={'source': 'docs/attention.pdf', 'page': 11}, page_content='summarization. arXiv preprint arXiv:1705.04304 , 2017.'), Document(metadata={'source': 'docs/attention.pdf', 'page': 11}, page_content='[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,'), Document(metadata={'source': 'docs/attention.pdf', 'page': 11}, page_content='and interpretable tree annotation. In Proceedings of the 21st International Conference on'), Document(metadata={'source': 'docs/attention.pdf', 'page': 11}, page_content='Computational Linguistics and 44th Annual Meeting of the ACL , pages 433–440. ACL, July\\n2006.'), Document(metadata={'source': 'docs/attention.pdf', 'page': 11}, page_content='2006.\\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv'), Document(metadata={'source': 'docs/attention.pdf', 'page': 11}, page_content='preprint arXiv:1608.05859 , 2016.'), Document(metadata={'source': 'docs/attention.pdf', 'page': 11}, page_content='[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words'), Document(metadata={'source': 'docs/attention.pdf', 'page': 11}, page_content='with subword units. arXiv preprint arXiv:1508.07909 , 2015.'), Document(metadata={'source': 'docs/attention.pdf', 'page': 11}, page_content='[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,'), Document(metadata={'source': 'docs/attention.pdf', 'page': 11}, page_content='and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts'), Document(metadata={'source': 'docs/attention.pdf', 'page': 11}, page_content='layer. arXiv preprint arXiv:1701.06538 , 2017.'), Document(metadata={'source': 'docs/attention.pdf', 'page': 11}, page_content='[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-'), Document(metadata={'source': 'docs/attention.pdf', 'page': 11}, page_content='nov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine'), Document(metadata={'source': 'docs/attention.pdf', 'page': 11}, page_content='Learning Research , 15(1):1929–1958, 2014.'), Document(metadata={'source': 'docs/attention.pdf', 'page': 11}, page_content='[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory'), Document(metadata={'source': 'docs/attention.pdf', 'page': 11}, page_content='networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,'), Document(metadata={'source': 'docs/attention.pdf', 'page': 11}, page_content='Advances in Neural Information Processing Systems 28 , pages 2440–2448. Curran Associates,'), Document(metadata={'source': 'docs/attention.pdf', 'page': 11}, page_content='Inc., 2015.'), Document(metadata={'source': 'docs/attention.pdf', 'page': 11}, page_content='[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural'), Document(metadata={'source': 'docs/attention.pdf', 'page': 11}, page_content='networks. In Advances in Neural Information Processing Systems , pages 3104–3112, 2014.'), Document(metadata={'source': 'docs/attention.pdf', 'page': 11}, page_content='[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.'), Document(metadata={'source': 'docs/attention.pdf', 'page': 11}, page_content='Rethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.'), Document(metadata={'source': 'docs/attention.pdf', 'page': 11}, page_content='[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In'), Document(metadata={'source': 'docs/attention.pdf', 'page': 11}, page_content='Advances in Neural Information Processing Systems , 2015.'), Document(metadata={'source': 'docs/attention.pdf', 'page': 11}, page_content='[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang'), Document(metadata={'source': 'docs/attention.pdf', 'page': 11}, page_content='Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine'), Document(metadata={'source': 'docs/attention.pdf', 'page': 11}, page_content='translation system: Bridging the gap between human and machine translation. arXiv preprint'), Document(metadata={'source': 'docs/attention.pdf', 'page': 11}, page_content='arXiv:1609.08144 , 2016.'), Document(metadata={'source': 'docs/attention.pdf', 'page': 11}, page_content='[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with'), Document(metadata={'source': 'docs/attention.pdf', 'page': 11}, page_content='fast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.'), Document(metadata={'source': 'docs/attention.pdf', 'page': 11}, page_content='[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate'), Document(metadata={'source': 'docs/attention.pdf', 'page': 11}, page_content='shift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume'), Document(metadata={'source': 'docs/attention.pdf', 'page': 11}, page_content='1: Long Papers) , pages 434–443. ACL, August 2013.\\n12'), Document(metadata={'source': 'docs/attention.pdf', 'page': 12}, page_content='Attention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican'), Document(metadata={'source': 'docs/attention.pdf', 'page': 12}, page_content='of\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess'), Document(metadata={'source': 'docs/attention.pdf', 'page': 12}, page_content='or\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit'), Document(metadata={'source': 'docs/attention.pdf', 'page': 12}, page_content='is\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking'), Document(metadata={'source': 'docs/attention.pdf', 'page': 12}, page_content='since\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>'), Document(metadata={'source': 'docs/attention.pdf', 'page': 12}, page_content='<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>'), Document(metadata={'source': 'docs/attention.pdf', 'page': 12}, page_content='<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the'), Document(metadata={'source': 'docs/attention.pdf', 'page': 12}, page_content='encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency'), Document(metadata={'source': 'docs/attention.pdf', 'page': 12}, page_content='distant dependency of'), Document(metadata={'source': 'docs/attention.pdf', 'page': 12}, page_content='the verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for'), Document(metadata={'source': 'docs/attention.pdf', 'page': 12}, page_content='the word ‘making’. Different colors represent different heads. Best viewed in color.\\n13'), Document(metadata={'source': 'docs/attention.pdf', 'page': 13}, page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat'), Document(metadata={'source': 'docs/attention.pdf', 'page': 13}, page_content='just\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,'), Document(metadata={'source': 'docs/attention.pdf', 'page': 13}, page_content='never\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion'), Document(metadata={'source': 'docs/attention.pdf', 'page': 13}, page_content=',\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits'), Document(metadata={'source': 'docs/attention.pdf', 'page': 13}, page_content='perfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>'), Document(metadata={'source': 'docs/attention.pdf', 'page': 13}, page_content='my\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis'), Document(metadata={'source': 'docs/attention.pdf', 'page': 13}, page_content='be\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>'), Document(metadata={'source': 'docs/attention.pdf', 'page': 13}, page_content='<pad>Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora'), Document(metadata={'source': 'docs/attention.pdf', 'page': 13}, page_content='in anaphora resolution. Top:'), Document(metadata={'source': 'docs/attention.pdf', 'page': 13}, page_content='Full attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention'), Document(metadata={'source': 'docs/attention.pdf', 'page': 13}, page_content='‘its’ for attention heads 5'), Document(metadata={'source': 'docs/attention.pdf', 'page': 13}, page_content='and 6. Note that the attentions are very sharp for this word.\\n14'), Document(metadata={'source': 'docs/attention.pdf', 'page': 14}, page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat'), Document(metadata={'source': 'docs/attention.pdf', 'page': 14}, page_content='just\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,'), Document(metadata={'source': 'docs/attention.pdf', 'page': 14}, page_content='never\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion'), Document(metadata={'source': 'docs/attention.pdf', 'page': 14}, page_content=',\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits'), Document(metadata={'source': 'docs/attention.pdf', 'page': 14}, page_content='perfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>'), Document(metadata={'source': 'docs/attention.pdf', 'page': 14}, page_content='my\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis'), Document(metadata={'source': 'docs/attention.pdf', 'page': 14}, page_content='be\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>'), Document(metadata={'source': 'docs/attention.pdf', 'page': 14}, page_content='<pad>Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of'), Document(metadata={'source': 'docs/attention.pdf', 'page': 14}, page_content='to the structure of the'), Document(metadata={'source': 'docs/attention.pdf', 'page': 14}, page_content='sentence. We give two such examples above, from two different heads from the encoder self-attention'), Document(metadata={'source': 'docs/attention.pdf', 'page': 14}, page_content='at layer 5 of 6. The heads clearly learned to perform different tasks.\\n15')]\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# Load example document\n",
    "loader = PyPDFLoader('docs/attention.pdf')\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "texts = text_splitter.split_documents(docs)\n",
    "print(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='The world must be made safe for democracy. Its peace must be planted upon the tested foundations of'\n",
      "page_content='foundations of political liberty. We have no selfish ends to serve. We desire no conquest, no'\n"
     ]
    }
   ],
   "source": [
    "# Load example document\n",
    "with open(\"docs/speech.txt\") as f:\n",
    "    speech = f.read()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "texts = text_splitter.create_documents([speech])\n",
    "print(texts[0])\n",
    "print(texts[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
